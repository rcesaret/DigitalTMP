{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "papermill": {
     "duration": 0.002056,
     "end_time": "2025-07-06T17:32:23.543125",
     "exception": false,
     "start_time": "2025-07-06T17:32:23.541069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Comparative Analysis of TMP Databases\n",
    "\n",
    "**Project:** Digital Teotihuacan Mapping Project (TMP) - Phase 1\n",
    "\n",
    "**Objective:** This notebook synthesizes the results from the entire profiling pipeline to conduct a comparative analysis of the four legacy databases and the two wide-format benchmark databases. Its primary goal is to use quantitative data to compare these database architectures on three key axes: **Structural Complexity**, **Resource Usage**, and **Query Performance**. \n",
    "\n",
    "The findings from this notebook will directly inform the final recommendation for the Phase 2 unified database architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "papermill": {
     "duration": 0.003299,
     "end_time": "2025-07-06T17:32:23.549928",
     "exception": false,
     "start_time": "2025-07-06T17:32:23.546629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "--- \n",
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "papermill": {
     "duration": 1.630671,
     "end_time": "2025-07-06T17:32:25.183055",
     "exception": false,
     "start_time": "2025-07-06T17:32:23.552384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "# --- Path Definitions ---\n",
    "# Use more robust path construction based on notebook file location\n",
    "try:\n",
    "    # Try to get notebook's directory (works in most Jupyter environments)\n",
    "    NOTEBOOK_DIR = Path(os.getcwd())\n",
    "    # Find the project root by looking for specific markers\n",
    "    current_path = NOTEBOOK_DIR\n",
    "    while current_path != current_path.parent:\n",
    "        if (current_path / \"TASKS.md\").exists() or (current_path / \"pyproject.toml\").exists():\n",
    "            PROJECT_ROOT = current_path\n",
    "            break\n",
    "        current_path = current_path.parent\n",
    "    else:\n",
    "        # Fallback: assume standard structure\n",
    "        PROJECT_ROOT = NOTEBOOK_DIR.parent.parent.parent.parent\n",
    "    \n",
    "    REPORTS_DIR = PROJECT_ROOT / \"phases\" / \"01_LegacyDB\" / \"outputs\" / \"reports\"\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Path detection failed ({e}). Using fallback path construction.\")\n",
    "    # Fallback to original approach\n",
    "    PROJECT_ROOT = Path.cwd().parent.parent\n",
    "    REPORTS_DIR = PROJECT_ROOT / \"outputs\" / \"reports\"\n",
    "\n",
    "# --- Styling and Display Options ---\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "def display_header(title):\n",
    "    display(Markdown(f'### {title}'))\n",
    "\n",
    "print(\"‚úÖ Setup complete.\")\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Reports Directory: {REPORTS_DIR}\")\n",
    "print(f\"Reports Directory exists: {REPORTS_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "papermill": {
     "duration": 0.003427,
     "end_time": "2025-07-06T17:32:25.190004",
     "exception": false,
     "start_time": "2025-07-06T17:32:25.186577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "--- \n",
    "## 2. Data Loading\n",
    "We load two key outputs from the `04_run_comparison.py` script:\n",
    "1. `comparison_matrix.csv`: High-level summary metrics.\n",
    "2. `report_performance_summary_detailed.csv`: The enriched, long-form performance data with calculated comparative metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "papermill": {
     "duration": 0.344415,
     "end_time": "2025-07-06T17:32:25.536704",
     "exception": true,
     "start_time": "2025-07-06T17:32:25.192289",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix_path = REPORTS_DIR / 'comparison_matrix.csv'\n",
    "perf_path = REPORTS_DIR / 'report_performance_summary_detailed.csv'\n",
    "\n",
    "# Validate file existence with detailed error reporting\n",
    "missing_files = []\n",
    "if not matrix_path.exists():\n",
    "    missing_files.append(str(matrix_path))\n",
    "if not perf_path.exists():\n",
    "    missing_files.append(str(perf_path))\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"Directory contents: {list(REPORTS_DIR.glob('*')) if REPORTS_DIR.exists() else 'Directory does not exist'}\")\n",
    "    raise FileNotFoundError(f\"Critical Error: The following report files were not found:\\n\" + \n",
    "                           \"\\n\".join(f\"  - {f}\" for f in missing_files) + \n",
    "                           f\"\\n\\nSearched in: {REPORTS_DIR}\\n\" +\n",
    "                           \"Please run the 04_run_comparison.py script first to generate these files.\")\n",
    "\n",
    "try:\n",
    "    # Load the matrix and transpose it so databases are rows\n",
    "    comparison_df = pd.read_csv(matrix_path, index_col=0).T.reset_index().rename(columns={'index': 'Database'})\n",
    "    print(f\"‚úÖ Successfully loaded comparison matrix: {comparison_df.shape[0]} databases, {comparison_df.shape[1]} metrics\")\n",
    "    \n",
    "    # Load the detailed performance data\n",
    "    perf_summary_df = pd.read_csv(perf_path)\n",
    "    print(f\"‚úÖ Successfully loaded performance data: {perf_summary_df.shape[0]} records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data files: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nLoaded Comparison Matrix:\")\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\nLoaded Detailed Performance Summary Data:\")\n",
    "display(perf_summary_df.head())\n",
    "print(f\"\\nPerformance data columns: {list(perf_summary_df.columns)}\")\n",
    "print(f\"Unique databases in performance data: {perf_summary_df['database'].unique() if 'database' in perf_summary_df.columns else 'No database column found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "--- \n",
    "## 3. High-Level Comparison Matrix\n",
    "A styled view of the main comparison matrix. Color gradients highlight high/low values for each metric, providing an at-a-glance summary.\n",
    "- <span style='color: #440154;'>**Purple/Dark**</span>: Higher values\n",
    "- <span style='color: #fde725;'>**Yellow/Light**</span>: Lower values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_header(\"Styled Comparison Matrix\")\n",
    "\n",
    "styled_df = comparison_df.style.background_gradient(cmap='viridis', axis=0)\\\n",
    "    .set_caption(\"Comparative Database Metrics\")\\\n",
    "    .format('{:.2f}', subset=pd.IndexSlice[:, ['Database Size (MB)', 'JDI (Join Dependency Index)', 'NF (Normalization Factor)']])\\\n",
    "    .format('{:,.0f}', subset=pd.IndexSlice[:, ['Table Count', 'View Count', 'Total Estimated Rows', 'Total Index Count', 'LIF (Logical Interop. Factor)']])\n",
    "\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "--- \n",
    "## 4. Structural Complexity Analysis\n",
    "This section focuses on the metrics that quantify the relational complexity and degree of normalization of the legacy schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_header(\"Schema Complexity Metrics (Legacy Databases)\")\n",
    "\n",
    "complexity_metrics = [\n",
    "    'Database', 'Table Count', \n",
    "    'JDI (Join Dependency Index)', \n",
    "    'LIF (Logical Interop. Factor)',\n",
    "    'NF (Normalization Factor)'\n",
    "]\n",
    "# Filter for legacy DBs only, as these metrics don't apply to the single-table benchmarks\n",
    "legacy_df = comparison_df[~comparison_df['Database'].str.contains('benchmark')]\n",
    "display(legacy_df[complexity_metrics])\n",
    "\n",
    "# --- Advanced Visualization: Complexity Radar Plot ---\n",
    "radar_metrics = ['Table Count', 'JDI (Join Dependency Index)', 'NF (Normalization Factor)']\n",
    "radar_df = legacy_df[['Database'] + radar_metrics].copy()\n",
    "\n",
    "# Normalize metrics to a 0-1 scale for fair comparison on the radar plot\n",
    "scaler = MinMaxScaler()\n",
    "radar_df[radar_metrics] = scaler.fit_transform(radar_df[radar_metrics])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for index, row in radar_df.iterrows():\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=row[radar_metrics].values,\n",
    "        theta=radar_metrics,\n",
    "        fill='toself',\n",
    "        name=row['Database']\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "  polar=dict(\n",
    "    radialaxis=dict(\n",
    "      visible=True,\n",
    "      range=[0, 1]\n",
    "    )),\n",
    "  showlegend=True,\n",
    "  title='Normalized Complexity Profile of Legacy Databases'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "--- \n",
    "## 5. Query Performance Deep Dive\n",
    "This is the most critical comparison. It directly measures the analytical query performance of the legacy normalized schemas against the denormalized wide-format benchmark schemas using the pre-calculated metrics from the `04_run_comparison.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_header(\"Schema Efficiency Factor by Query Category\")\n",
    "\n",
    "if not perf_summary_df.empty:\n",
    "    # First, display the tabular data (NOT logarithmic)\n",
    "    print(\"üìä Schema Efficiency Factor Data Table (Raw Values):\")\n",
    "    print(\"Note: Values > 1.0 indicate the database is slower than the benchmark baseline\")\n",
    "    \n",
    "    # Create a pivot table for better readability\n",
    "    if 'schema_efficiency_factor' in perf_summary_df.columns:\n",
    "        efficiency_table = perf_summary_df.pivot_table(\n",
    "            index='database', \n",
    "            columns='category', \n",
    "            values='schema_efficiency_factor',\n",
    "            aggfunc='mean'\n",
    "        ).round(2)\n",
    "        \n",
    "        # Add a summary column showing average across categories\n",
    "        efficiency_table['Average'] = efficiency_table.mean(axis=1).round(2)\n",
    "        \n",
    "        # Style the table to highlight high values\n",
    "        styled_efficiency = efficiency_table.style.background_gradient(cmap='Reds', axis=None)\\\n",
    "            .set_caption(\"Schema Efficiency Factors (Lower is Better - Benchmark = 1.0)\")\\\n",
    "            .format('{:.2f}')\n",
    "        \n",
    "        display(styled_efficiency)\n",
    "        \n",
    "        # Also show summary statistics\n",
    "        print(f\"\\nüìà Summary Statistics:\")\n",
    "        print(f\"Worst performing database (highest average): {efficiency_table['Average'].idxmax()} ({efficiency_table['Average'].max():.2f}x slower)\")\n",
    "        print(f\"Best performing database (lowest average): {efficiency_table['Average'].idxmin()} ({efficiency_table['Average'].min():.2f}x)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è 'schema_efficiency_factor' column not found in performance data\")\n",
    "        display(perf_summary_df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä Schema Efficiency Factor Chart (Log Scale):\")\n",
    "    \n",
    "    # Then display the chart with log scale\n",
    "    fig = px.bar(perf_summary_df,\n",
    "                 x='database', \n",
    "                 y='schema_efficiency_factor', \n",
    "                 color='category', \n",
    "                 barmode='group',\n",
    "                 title='Schema Efficiency Factor (Lower is Better)',\n",
    "                 labels={'schema_efficiency_factor': 'Efficiency Factor (Log Scale)', 'database': 'Database'},\n",
    "                 category_orders={'category': ['baseline', 'join_performance', 'complex_filtering']})\n",
    "    \n",
    "    fig.update_yaxes(type=\"log\") # Use a log scale as differences can be huge\n",
    "    fig.add_hline(y=1.0, line_dash=\"dot\", annotation_text=\"Benchmark Baseline\", annotation_position=\"bottom right\")\n",
    "    fig.update_layout(height=600)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No performance data to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_header(\"Performance Improvement vs. Best Benchmark\")\n",
    "\n",
    "if not perf_summary_df.empty:\n",
    "    # First, display the tabular data\n",
    "    print(\"üìä Performance Improvement Data Table:\")\n",
    "    print(\"Note: Positive values indicate how much faster the benchmark is compared to legacy databases\")\n",
    "    \n",
    "    # Filter out the baseline databases themselves for cleaner analysis\n",
    "    improvement_df = perf_summary_df[~perf_summary_df['database'].str.contains('benchmark')].copy()\n",
    "    \n",
    "    if not improvement_df.empty and 'performance_improvement_factor' in improvement_df.columns:\n",
    "        # Create a summary table showing improvement factors\n",
    "        improvement_summary = improvement_df.groupby(['database', 'category'])['performance_improvement_factor'].agg(['mean', 'min', 'max']).round(1)\n",
    "        improvement_summary.columns = ['Avg_Improvement_%', 'Min_Improvement_%', 'Max_Improvement_%']\n",
    "        \n",
    "        # Reset index to make it more readable\n",
    "        improvement_summary = improvement_summary.reset_index()\n",
    "        improvement_pivot = improvement_summary.pivot(index='database', columns='category', values='Avg_Improvement_%').round(1)\n",
    "        \n",
    "        # Style the table\n",
    "        styled_improvement = improvement_pivot.style.background_gradient(cmap='Greens', axis=None)\\\n",
    "            .set_caption(\"Average Performance Improvement (% faster than benchmark)\")\\\n",
    "            .format('{:.1f}%')\n",
    "        \n",
    "        display(styled_improvement)\n",
    "        \n",
    "        # Show detailed breakdown\n",
    "        print(f\"\\nüìã Detailed Performance Improvement Breakdown:\")\n",
    "        detailed_table = improvement_df[['database', 'category', 'query_id', 'performance_improvement_factor']].copy()\n",
    "        detailed_table['performance_improvement_factor'] = detailed_table['performance_improvement_factor'].round(1)\n",
    "        detailed_table = detailed_table.rename(columns={\n",
    "            'performance_improvement_factor': 'Improvement_%'\n",
    "        })\n",
    "        \n",
    "        # Sort by improvement factor for better readability\n",
    "        detailed_table = detailed_table.sort_values(['database', 'category', 'Improvement_%'], ascending=[True, True, False])\n",
    "        display(detailed_table)\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\nüìà Key Insights:\")\n",
    "        best_db = improvement_pivot.mean(axis=1).idxmin()\n",
    "        worst_db = improvement_pivot.mean(axis=1).idxmax()\n",
    "        print(f\"Most consistent performer: {best_db} (avg {improvement_pivot.mean(axis=1)[best_db]:.1f}% improvement over benchmark)\")\n",
    "        print(f\"Least consistent performer: {worst_db} (avg {improvement_pivot.mean(axis=1)[worst_db]:.1f}% improvement over benchmark)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è 'performance_improvement_factor' column not found or no legacy database data available\")\n",
    "        if not improvement_df.empty:\n",
    "            display(improvement_df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä Performance Improvement Chart:\")\n",
    "    \n",
    "    # Then display the chart\n",
    "    fig = px.bar(improvement_df.sort_values('performance_improvement_factor') if not improvement_df.empty else improvement_df,\n",
    "                 x='query_id', \n",
    "                 y='performance_improvement_factor', \n",
    "                 color='database', \n",
    "                 facet_row='category',\n",
    "                 barmode='group',\n",
    "                 title='Performance Improvement of Benchmark Schemas vs. Legacy Schemas',\n",
    "                 labels={'performance_improvement_factor': '% Improvement vs. Benchmark', 'query_id': 'Query ID'})\n",
    "    \n",
    "    fig.update_layout(height=800)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Could not generate performance improvement plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "--- \n",
    "## 6. Qualitative Architectural Trade-offs\n",
    "The quantitative data above supports a qualitative assessment of the architectural trade-offs between the legacy design and the proposed wide-format design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "| Feature                  | Legacy Normalized (e.g., DF9)      | Proposed Wide-Format (Benchmark)   | Justification Based on Data                                                                      |\n",
    "| :----------------------- | :--------------------------------- | :--------------------------------- | :----------------------------------------------------------------------------------------------- |\n",
    "| **Query Performance** | `Low`                              | `High`                             | The 'Schema Efficiency Factor' chart shows legacy databases are multiple times slower.             |\n",
    "| **Storage Cost** | `Low`                              | `High`                             | `comparison_matrix.csv` shows benchmark DBs are larger due to data duplication.                |\n",
    "| **Schema Complexity** | `High` (High JDI/NF, Many Tables)  | `Very Low` (1 Table)               | The complexity radar plot visually confirms the high complexity scores of the legacy schemas.    |\n",
    "| **Data Redundancy** | `Low` (Normalized)                 | `High` (Denormalized)              | This is the inherent trade-off of the wide-format design; we trade storage for speed.            |\n",
    "| **Ease of Use for BI/GIS** | `Low` (Requires complex joins)     | `High` (Single table source)       | A single flat table is trivial to connect to tools like QGIS, Tableau, or Power BI.              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## 7. Final Analyst Summary & Recommendation\n",
    "\n",
    "**Instructions:** Based on the comparative analysis, synthesize the findings and provide a formal recommendation for the Phase 2 unified database architecture. This summary will be a primary input for the final white paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Overarching Conclusion:\n",
    "* *Start with a concise, definitive statement. Example: \"The comparative analysis demonstrates conclusively that the highly normalized structure of the legacy databases, particularly `tmp_df9`, is quantitatively inferior for the project's analytical objectives compared to a denormalized, wide-format architecture.\"*\n",
    "\n",
    "### Justification from Evidence:\n",
    "1.  **On Performance:**\n",
    "    * *Quantify the performance difference. Reference the 'Schema Efficiency Factor' chart directly. Example: \"As shown in the efficiency factor plot, the legacy schemas are between 5x and 50x slower for join-heavy queries than the wide-format benchmarks. This performance gap makes interactive analysis on the normalized schemas untenable.\"*\n",
    "2.  **On Complexity:**\n",
    "    * *Reference the complexity metrics and the radar plot. Example: \"The legacy schemas exhibit high JDI and NF scores, indicative of significant relational complexity that increases the cognitive load for analysts and the technical barrier for connecting to BI and GIS tools. The radar plot clearly visualizes `tmp_df9` as the most complex outlier.\"*\n",
    "3.  **On The Cost/Benefit Trade-off:**\n",
    "    * *Acknowledge the trade-offs identified in the qualitative table. Example: \"While the wide-format approach increases storage costs due to data redundancy, this trade-off is strategically acceptable. The cost of storage is minimal compared to the significant gains in query performance and the drastic reduction in development time and analytical friction for end-users.\"*\n",
    "\n",
    "### Formal Recommendation:\n",
    "* *State the final recommendation clearly and unambiguously.*\n",
    "* **Recommended Architecture:** \"It is the formal recommendation of this analysis that Phase 2 of the Digital TMP project proceeds with the development of a single, denormalized, wide-format primary analytical table. This table should be based on the schema of the `tmp_benchmark_wide_text_nulls` database, as it provides the best balance of performance and human-readability.\"*\n",
    "* **Next Steps:** \"The next step should be to finalize the schema of this wide-format table, including data type assignments and column naming conventions, and to proceed with the development of the full ETL pipeline in Phase 2 to migrate all legacy data into this new structure.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digital_tmp_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.034093,
   "end_time": "2025-07-06T17:32:25.889765",
   "environment_variables": {},
   "exception": true,
   "input_path": "c:\\Users\\rcesa\\ASU Dropbox\\Rudolf Cesaretti\\GitHubRepos\\TeoMappingProject\\phases\\01_LegacyDB\\reports\\individual_db_analysis\\06_Comparative_Analysis_Report.ipynb",
   "output_path": "c:\\Users\\rcesa\\ASU Dropbox\\Rudolf Cesaretti\\GitHubRepos\\TeoMappingProject\\phases\\01_LegacyDB\\reports\\individual_db_analysis\\06_Comparative_Analysis_Report.ipynb",
   "parameters": {},
   "start_time": "2025-07-06T17:32:21.855672",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
